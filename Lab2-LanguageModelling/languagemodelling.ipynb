{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 2: Language Modelling\n",
    "\n",
    "## 1. Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, random, math, operator\n",
    "TRAINING_DIR = \"./sentence-completion/Holmes_Training_Data\"  # this needs to be the parent directory for the training corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 137,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_training_testing(training_dir=TRAINING_DIR, split=0.5):\n",
    "    filenames = os.listdir(training_dir)\n",
    "    n = len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n, training_dir))\n",
    "    random.seed(53) #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index = int(n * split)\n",
    "    return filenames[:index], filenames[index:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: ./sentence-completion/Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "trainingfiles, heldoutfiles = get_training_testing()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. A Unigram Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as tokenize\n",
    "\n",
    "class language_model:\n",
    "\n",
    "    def __init__(self, trainingdir=TRAINING_DIR, files=[]):\n",
    "        self.training_dir = trainingdir\n",
    "        self.files = files\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self._processfiles()\n",
    "        self._convert_to_probs()\n",
    "\n",
    "    def _processline(self,line):\n",
    "        tokens=[\"_START\"]+tokenize(line)+[\"_END\"]\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "\n",
    "    def _processfiles(self):\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir, afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line) > 0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "\n",
    "    def _convert_to_probs(self):\n",
    "        self.unigram = {k: v / sum(self.unigram.values()) for (k, v) in self.unigram.items()}\n",
    "\n",
    "    def get_prob(self, token, method=\"unigram\"):\n",
    "        if method == \"unigram\":\n",
    "            return self.unigram.get(token, 0)\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "        return 0\n",
    "\n",
    "    # 2.2 generation\n",
    "    def generate_simple_sentence(self, k=5, limit=15):\n",
    "        top_k = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "        sentence = []\n",
    "        token = ''\n",
    "        while token!='_END' and len(sentence) < limit:\n",
    "            top_k = [(tok, prob) for (tok, prob) in top_k if tok != '_START']\n",
    "            token = random.choice(top_k)[0]\n",
    "            sentence.append(token)\n",
    "        return ' '.join(sentence[:-1])\n",
    "\n",
    "    def generate_sentence_extension(self, limit=15):\n",
    "        \"\"\"\n",
    "        Using the cumulative probability distribution (with random.choices())\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sentence = []\n",
    "        token = ''\n",
    "        keys = list(self.unigram.keys())\n",
    "        values = list(self.unigram.values())\n",
    "        while token != '.' and len(sentence) < limit:\n",
    "            token = random.choices(keys, values)\n",
    "            if token[0] != '_START' and token[0] != '_END':\n",
    "                sentence.append(token)\n",
    "        return ' '.join(token[0] for token in sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1. Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n"
     ]
    }
   ],
   "source": [
    "MAX_FILES=5\n",
    "mylm = language_model(files=trainingfiles[:MAX_FILES])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "6.751525844840934e-06"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_prob('tales')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "0.00012377797382208378"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_prob('case')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "sorted_unigram_tuples = sorted(mylm.unigram.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_unigram = {k: v for k, v in sorted_unigram_tuples}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def generate_simple_sentence():\n",
    "    top_10 = sorted_unigram_tuples[:10]\n",
    "    sentence = []\n",
    "    token = ''\n",
    "    while token != '_END' and len(sentence) < 15:\n",
    "        top_10 = [(tok, prob) for (tok, prob) in top_10 if tok != '_START']\n",
    "        token = random.choice(top_10)[0]\n",
    "        sentence.append(token)\n",
    "    return ' '.join(sentence[:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "'the , the `` `` the'"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sentence = generate_simple_sentence()\n",
    "simple_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "', the'"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.generate_simple_sentence()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "\"`` the the that his you that the and ''\""
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.generate_simple_sentence(k=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['departure']\n"
     ]
    }
   ],
   "source": [
    "print(random.choices(list(mylm.unigram.keys()), list(mylm.unigram.values())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def generate_sentence_extension():\n",
    "    sentence = []\n",
    "    token = ''\n",
    "    while token != '.' and len(sentence) < 15:\n",
    "        token = random.choices(list(mylm.unigram.keys()), list(mylm.unigram.values()))\n",
    "        if token[0] != '_START' and token[0] != '_END':\n",
    "            sentence.append(token)\n",
    "    return ' '.join(token[0] for token in sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "'; I him back was the light , up . manage each his , to'"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence_extension()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "\"of . making `` you '' ask Cross breezily to serious coming . he and\""
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.generate_sentence_extension()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Adding Bigrams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class language_model:\n",
    "\n",
    "    def __init__(self, trainingdir=TRAINING_DIR, files=[]):\n",
    "        self.training_dir = trainingdir\n",
    "        self.files = files\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self._processfiles()\n",
    "        self._convert_to_probs()\n",
    "\n",
    "    def _processline(self, line):\n",
    "        tokens = [\"_START\"] + tokenize(line) + [\"_END\"]\n",
    "        for i, token in enumerate(tokens):\n",
    "            self.unigram[token] = self.unigram.get(token, 0) + 1\n",
    "            if i > 0:\n",
    "                previous = tokens[i-1]\n",
    "                current = self.bigram.get(previous, {})\n",
    "                current[token] = current.get(token, 0) + 1\n",
    "                self.bigram[previous] = current\n",
    "\n",
    "    def _processfiles(self):\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir, afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line) > 0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "\n",
    "    def _convert_to_probs(self):\n",
    "        self.unigram = {k: v / sum(self.unigram.values()) for (k, v) in self.unigram.items()}\n",
    "        for (k, v) in self.bigram.items():\n",
    "            self.bigram[k] = {in_k: in_v / sum(v.values()) for (in_k, in_v) in v.items()}\n",
    "\n",
    "    def get_prob(self, token, previous_token=\"\", method=\"unigram\"):\n",
    "        if method == \"unigram\":\n",
    "            return self.unigram.get(token, 0)\n",
    "        elif method == 'bigram':\n",
    "            return self.bigram.get(previous_token, {}).get(token, 0)\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "        return 0\n",
    "\n",
    "    # 2.2 generation\n",
    "    def generate_simple_sentence(self, k=5, limit=15):\n",
    "        top_k = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "        sentence = []\n",
    "        token = ''\n",
    "        while token!='_END' and len(sentence) < limit:\n",
    "            top_k = [(tok, prob) for (tok, prob) in top_k if tok != '_START']\n",
    "            token = random.choice(top_k)[0]\n",
    "            sentence.append(token)\n",
    "        return ' '.join(sentence[:-1])\n",
    "\n",
    "    def generate_sentence_extension(self, limit=15):\n",
    "        \"\"\"\n",
    "        Using the cumulative probability distribution (with random.choices())\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sentence = []\n",
    "        token = ''\n",
    "        keys = list(self.unigram.keys())\n",
    "        values = list(self.unigram.values())\n",
    "        while token != '.' and len(sentence) < limit:\n",
    "            token = random.choices(keys, values)\n",
    "            if token[0] != '_START' and token[0] != '_END':\n",
    "                sentence.append(token)\n",
    "        return ' '.join(token[0] for token in sentence)\n",
    "\n",
    "    # 3.2 bigram generation\n",
    "    def generate_sentence_from_bigram(self, k=5, limit=15):\n",
    "        sentence = []\n",
    "        token = '_START'\n",
    "        while token != '_END' and len(sentence) < limit: # and token != '.'\n",
    "            possibilities = self.bigram.get(token, {})\n",
    "            top = sorted(possibilities.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "            token = random.choice(top)[0]\n",
    "            sentence.append(token)\n",
    "        return ' '.join(sentence[:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1. Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n"
     ]
    }
   ],
   "source": [
    "MAX_FILES=5\n",
    "mylm = language_model(files=trainingfiles[:MAX_FILES])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "0.012881010075245504"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_prob('The', '_START', method='bigram')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0026893577948616386"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_prob('The', method='unigram')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2. Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "'the wind , as he had the wind came from her eyes of that'"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.generate_sentence_from_bigram(k=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 Perplexity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "class language_model:\n",
    "\n",
    "    def __init__(self, trainingdir=TRAINING_DIR, files=[]):\n",
    "        self.training_dir = trainingdir\n",
    "        self.files = files\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self._processfiles()\n",
    "        self._convert_to_probs()\n",
    "\n",
    "    def _processline(self, line):\n",
    "        tokens = [\"_START\"] + tokenize(line) + [\"_END\"]\n",
    "        for i, token in enumerate(tokens):\n",
    "            self.unigram[token] = self.unigram.get(token, 0) + 1\n",
    "            if i > 0:\n",
    "                previous = tokens[i-1]\n",
    "                current = self.bigram.get(previous, {})\n",
    "                current[token] = current.get(token, 0) + 1\n",
    "                self.bigram[previous] = current\n",
    "\n",
    "    def _processfiles(self):\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir, afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line) > 0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "\n",
    "    def _convert_to_probs(self):\n",
    "        self.unigram = {k: v / sum(self.unigram.values()) for (k, v) in self.unigram.items()}\n",
    "        for (k, v) in self.bigram.items():\n",
    "            self.bigram[k] = {in_k: in_v / sum(v.values()) for (in_k, in_v) in v.items()}\n",
    "\n",
    "    def get_prob(self, token, previous_token=\"\", method=\"unigram\"):\n",
    "        if method == \"unigram\":\n",
    "            return self.unigram.get(token, 0)\n",
    "        elif method == 'bigram':\n",
    "            return self.bigram.get(previous_token, {}).get(token, 0)\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "        return 0\n",
    "\n",
    "    # 2.2 generation\n",
    "    def generate_simple_sentence(self, k=5, limit=15):\n",
    "        top_k = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "        sentence = []\n",
    "        token = ''\n",
    "        while token!='_END' and len(sentence) < limit:\n",
    "            top_k = [(tok, prob) for (tok, prob) in top_k if tok != '_START']\n",
    "            token = random.choice(top_k)[0]\n",
    "            sentence.append(token)\n",
    "        return ' '.join(sentence[:-1])\n",
    "\n",
    "    def generate_sentence_extension(self, limit=15):\n",
    "        \"\"\"\n",
    "        Using the cumulative probability distribution (with random.choices())\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sentence = []\n",
    "        token = ''\n",
    "        keys = list(self.unigram.keys())\n",
    "        values = list(self.unigram.values())\n",
    "        while token != '.' and len(sentence) < limit:\n",
    "            token = random.choices(keys, values)\n",
    "            if token[0] != '_START' and token[0] != '_END':\n",
    "                sentence.append(token)\n",
    "        return ' '.join(token[0] for token in sentence)\n",
    "\n",
    "    # 3.2 bigram generation\n",
    "    def generate_sentence_from_bigram(self, k=5, limit=15):\n",
    "        sentence = []\n",
    "        token = '_START'\n",
    "        while token != '_END' and len(sentence) < limit: # and token != '.'\n",
    "            possibilities = self.bigram.get(token, {})\n",
    "            top = sorted(possibilities.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "            token = random.choice(top)[0]\n",
    "            sentence.append(token)\n",
    "        return ' '.join(sentence[:-1])\n",
    "\n",
    "    # 4 perplexity\n",
    "    # def get_log_probability(self, file, method='unigram'):\n",
    "    #     log_probs = 0\n",
    "    #     try:\n",
    "    #         with open(os.path.join(self.training_dir, file)) as instream:\n",
    "    #             if method == 'unigram':\n",
    "    #                 for line in instream:\n",
    "    #                     line = line.rstrip()\n",
    "    #                     if len(line) > 0:\n",
    "    #                         tokens = [\"_START\"] + tokenize(line) + [\"_END\"]\n",
    "    #                         log_probs += sum([math.log(self.get_prob(token, method='unigram')) for token in tokens])\n",
    "    #             elif method == 'bigram':\n",
    "    #                 for line in instream:\n",
    "    #                     line = line.rstrip()\n",
    "    #                     if len(line) > 0:\n",
    "    #                         tokens = [\"_START\"] + tokenize(line) + [\"_END\"]\n",
    "    #                         # log_probs += sum([math.log(self.get_prob(tokens[i+1], previous_token=token, method='bigram'))\n",
    "    #                         #                   for i, token in enumerate(tokens)])\n",
    "    #                         for i, token in enumerate(tokens):\n",
    "    #                             if i > 0:\n",
    "    #                                 log_probs += math.log(self.get_prob(token, previous_token=tokens[i-1], method='bigram'))\n",
    "    #\n",
    "    #     except UnicodeDecodeError:\n",
    "    #         print(\"UnicodeDecodeError processing {}: ignoring file\".format(file))\n",
    "    #     return log_probs\n",
    "    #\n",
    "    # def get_perplexity(self, file, method='unigram'):\n",
    "    #     text = open(os.path.join(self.training_dir, file)).read()\n",
    "    #     num_of_words = len(text.split())\n",
    "    #     log_probability = self.get_log_probability(file, method=method)\n",
    "    #     return math.exp(-log_probability/num_of_words)\n",
    "\n",
    "    def compute_prob_line(self,line,method=\"unigram\"):\n",
    "        #this will add _start to the beginning of a line of text\n",
    "        #compute the probability of the line according to the desired model\n",
    "        #and returns probability together with number of tokens\n",
    "\n",
    "        tokens=[\"_START\"]+tokenize(line)+[\"_END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens):\n",
    "            if i > 0:\n",
    "                acc+=math.log(self.get_prob(token,previous_token=tokens[i-1],method=method))\n",
    "        return acc,len(tokens[1:])\n",
    "\n",
    "\n",
    "    def compute_probability(self,filenames=[],method=\"unigram\"):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "\n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N=self.compute_prob_line(line,method=method)\n",
    "                            total_p+=p\n",
    "                            total_N+=N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "\n",
    "    def compute_perplexity(self,filenames=[],method=\"unigram\"):\n",
    "\n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "\n",
    "        p,N=self.compute_probability(filenames=filenames,method=method)\n",
    "        #print(p,N)\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n"
     ]
    }
   ],
   "source": [
    "MAX_FILES=5\n",
    "mylm = language_model(files=trainingfiles[:MAX_FILES])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "-666538.3915608346"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_log_probability('DYNMT10.TXT', method='unigram')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "5433.144986357501"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_perplexity('DYNMT10.TXT', method='unigram')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "-409464.2359516801"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_log_probability('DYNMT10.TXT', method='bigram')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "197.01367717606865"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_perplexity('DYNMT10.TXT', method='bigram')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "0.012881010075245504"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.get_prob('The', '_START', method='bigram')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-106-e79dd2c4d078>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmylm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_perplexity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'2DFRE10.TXT'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'bigram'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-98-223d6f3cdcfd>\u001B[0m in \u001B[0;36mget_perplexity\u001B[1;34m(self, file, method)\u001B[0m\n\u001B[0;32m    114\u001B[0m         \u001B[0mtext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraining_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m         \u001B[0mnum_of_words\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 116\u001B[1;33m         \u001B[0mlog_probability\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_log_probability\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    117\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexp\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mlog_probability\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mnum_of_words\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-98-223d6f3cdcfd>\u001B[0m in \u001B[0;36mget_log_probability\u001B[1;34m(self, file, method)\u001B[0m\n\u001B[0;32m    105\u001B[0m                             \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtoken\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    106\u001B[0m                                 \u001B[1;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 107\u001B[1;33m                                     \u001B[0mlog_probs\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_prob\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprevious_token\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'bigram'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    108\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    109\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mUnicodeDecodeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: math domain error"
     ]
    }
   ],
   "source": [
    "mylm.get_perplexity('2DFRE10.TXT', method='bigram')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0:DYNMT10.TXT\n",
      "Processing file 1:09TOM10.TXT\n",
      "Processing file 2:PRSIT10.TXT\n",
      "Processing file 3:NWIND10.TXT\n",
      "Processing file 4:BDAPH10.TXT\n"
     ]
    },
    {
     "data": {
      "text/plain": "487.82830040293857"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.compute_perplexity()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0:DYNMT10.TXT\n"
     ]
    },
    {
     "data": {
      "text/plain": "624.1897972356056"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.compute_perplexity(filenames=['DYNMT10.TXT'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0:DYNMT10.TXT\n"
     ]
    },
    {
     "data": {
      "text/plain": "59.133163959841994"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.compute_perplexity(filenames=['DYNMT10.TXT'], method='bigram')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Dealing with Unseen Data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0:GGIRL10.TXT\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-122-d3f6742c6cd7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmylm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_perplexity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilenames\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mheldoutfiles\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mMAX_FILES\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-117-39eabfc9fb54>\u001B[0m in \u001B[0;36mcompute_perplexity\u001B[1;34m(self, filenames, method)\u001B[0m\n\u001B[0;32m    157\u001B[0m         \u001B[1;31m#lower perplexity means that the model better explains the data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 159\u001B[1;33m         \u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mN\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_probability\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilenames\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfilenames\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    160\u001B[0m         \u001B[1;31m#print(p,N)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    161\u001B[0m         \u001B[0mpp\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexp\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mN\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-117-39eabfc9fb54>\u001B[0m in \u001B[0;36mcompute_probability\u001B[1;34m(self, filenames, method)\u001B[0m\n\u001B[0;32m    144\u001B[0m                         \u001B[0mline\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    145\u001B[0m                         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mline\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m>\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 146\u001B[1;33m                             \u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mN\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_prob_line\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mline\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    147\u001B[0m                             \u001B[0mtotal_p\u001B[0m\u001B[1;33m+=\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    148\u001B[0m                             \u001B[0mtotal_N\u001B[0m\u001B[1;33m+=\u001B[0m\u001B[0mN\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-117-39eabfc9fb54>\u001B[0m in \u001B[0;36mcompute_prob_line\u001B[1;34m(self, line, method)\u001B[0m\n\u001B[0;32m    126\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtoken\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    127\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 128\u001B[1;33m                 \u001B[0macc\u001B[0m\u001B[1;33m+=\u001B[0m\u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_prob\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mprevious_token\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    129\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0macc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: math domain error"
     ]
    }
   ],
   "source": [
    "mylm.compute_perplexity(filenames=heldoutfiles[:MAX_FILES])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1. Unknown Words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "# My class, not working properly\n",
    "class language_model:\n",
    "\n",
    "    def __init__(self, trainingdir=TRAINING_DIR, files=[]):\n",
    "        self.training_dir = trainingdir\n",
    "        self.files = files\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self._processfiles()\n",
    "        self.make_unknowns()\n",
    "        self._convert_to_probs()\n",
    "\n",
    "    def _processline(self, line):\n",
    "        tokens = [\"_START\"] + tokenize(line) + [\"_END\"]\n",
    "        for i, token in enumerate(tokens):\n",
    "            self.unigram[token] = self.unigram.get(token, 0) + 1\n",
    "            if i > 0:\n",
    "                previous = tokens[i-1]\n",
    "                current = self.bigram.get(previous, {})\n",
    "                current[token] = current.get(token, 0) + 1\n",
    "                self.bigram[previous] = current\n",
    "\n",
    "    def _processfiles(self):\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir, afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line) > 0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
    "\n",
    "    def _convert_to_probs(self):\n",
    "        self.unigram = {k: v / sum(self.unigram.values()) for (k, v) in self.unigram.items()}\n",
    "        for (k, v) in self.bigram.items():\n",
    "            self.bigram[k] = {in_k: in_v / sum(v.values()) for (in_k, in_v) in v.items()}\n",
    "\n",
    "    def get_prob(self, token, previous_token=\"\", method=\"unigram\"):\n",
    "        if method == \"unigram\":\n",
    "            return self.unigram.get(token, 0)\n",
    "        elif method == 'bigram':\n",
    "            bigram = self.bigram.get(previous_token, self.bigram.get(\"_UNK\",{}))\n",
    "            return bigram.get(token, bigram.get(\"__UNK\",0))\n",
    "        else:\n",
    "            print(\"Not implemented: {}\".format(method))\n",
    "        return 0\n",
    "\n",
    "    # 2.2 generation\n",
    "    def generate_simple_sentence(self, k=5, limit=15):\n",
    "        top_k = sorted(self.unigram.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "        sentence = []\n",
    "        token = ''\n",
    "        while token!='_END' and len(sentence) < limit:\n",
    "            top_k = [(tok, prob) for (tok, prob) in top_k if tok != '_START' and tok != '_UNK']\n",
    "            token = random.choice(top_k)[0]\n",
    "            sentence.append(token)\n",
    "        return ' '.join(sentence[:-1])\n",
    "\n",
    "    def generate_sentence_extension(self, limit=15):\n",
    "        \"\"\"\n",
    "        Using the cumulative probability distribution (with random.choices())\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sentence = []\n",
    "        token = ''\n",
    "        keys = list(self.unigram.keys())\n",
    "        values = list(self.unigram.values())\n",
    "        while token != '.' and len(sentence) < limit:\n",
    "            token = random.choices(keys, values)\n",
    "            if token[0] != '_START' and token[0] != '_END' and token[0] != '_UNK':\n",
    "                sentence.append(token)\n",
    "        return ' '.join(token[0] for token in sentence)\n",
    "\n",
    "    # 3.2 bigram generation\n",
    "    def generate_sentence_from_bigram(self, k=5, limit=15):\n",
    "        sentence = []\n",
    "        token = '_START'\n",
    "        while token != '_END' and len(sentence) < limit: # and token != '.'\n",
    "            possibilities = self.bigram.get(token, {})\n",
    "            top = sorted(possibilities.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "            token = random.choice(top)[0]\n",
    "            sentence.append(token)\n",
    "        return ' '.join(sentence[:-1])\n",
    "\n",
    "    # 4 perplexity\n",
    "    # def get_log_probability(self, file, method='unigram'):\n",
    "    #     log_probs = 0\n",
    "    #     try:\n",
    "    #         with open(os.path.join(self.training_dir, file)) as instream:\n",
    "    #             if method == 'unigram':\n",
    "    #                 for line in instream:\n",
    "    #                     line = line.rstrip()\n",
    "    #                     if len(line) > 0:\n",
    "    #                         tokens = [\"_START\"] + tokenize(line) + [\"_END\"]\n",
    "    #                         log_probs += sum([math.log(self.get_prob(token, method='unigram')) for token in tokens])\n",
    "    #             elif method == 'bigram':\n",
    "    #                 for line in instream:\n",
    "    #                     line = line.rstrip()\n",
    "    #                     if len(line) > 0:\n",
    "    #                         tokens = [\"_START\"] + tokenize(line) + [\"_END\"]\n",
    "    #                         # log_probs += sum([math.log(self.get_prob(tokens[i+1], previous_token=token, method='bigram'))\n",
    "    #                         #                   for i, token in enumerate(tokens)])\n",
    "    #                         for i, token in enumerate(tokens):\n",
    "    #                             if i > 0:\n",
    "    #                                 log_probs += math.log(self.get_prob(token, previous_token=tokens[i-1], method='bigram'))\n",
    "    #\n",
    "    #     except UnicodeDecodeError:\n",
    "    #         print(\"UnicodeDecodeError processing {}: ignoring file\".format(file))\n",
    "    #     return log_probs\n",
    "    #\n",
    "    # def get_perplexity(self, file, method='unigram'):\n",
    "    #     text = open(os.path.join(self.training_dir, file)).read()\n",
    "    #     num_of_words = len(text.split())\n",
    "    #     log_probability = self.get_log_probability(file, method=method)\n",
    "    #     return math.exp(-log_probability/num_of_words)\n",
    "\n",
    "    def compute_prob_line(self,line,method=\"unigram\"):\n",
    "        #this will add _start to the beginning of a line of text\n",
    "        #compute the probability of the line according to the desired model\n",
    "        #and returns probability together with number of tokens\n",
    "\n",
    "        tokens=[\"_START\"]+tokenize(line)+[\"_END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens):\n",
    "            if i > 0:\n",
    "                acc+=math.log(self.get_prob(token,previous_token=tokens[i-1],method=method))\n",
    "        return acc,len(tokens[1:])\n",
    "\n",
    "\n",
    "    def compute_probability(self,filenames=[],method=\"unigram\"):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "\n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N=self.compute_prob_line(line,method=method)\n",
    "                            total_p+=p\n",
    "                            total_N+=N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "\n",
    "    def compute_perplexity(self,filenames=[],method=\"unigram\"):\n",
    "\n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "\n",
    "        p,N=self.compute_probability(filenames=filenames,method=method)\n",
    "        #print(p,N)\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp\n",
    "\n",
    "    def make_unknowns(self, known=2):\n",
    "        for (k, v) in list(self.unigram.items()):\n",
    "            if v<known:\n",
    "                del self.unigram[k]\n",
    "                self.unigram[\"_UNK\"]=self.unigram.get(\"_UNK\",0)+v\n",
    "        for (k, adict) in list(self.bigram.items()):\n",
    "            for (kk, v) in list(adict.items()):\n",
    "                isknown=self.unigram.get(kk,0)\n",
    "                if isknown==0:\n",
    "                    adict[\"_UNK\"]=adict.get(\"_UNK\",0)+v\n",
    "                    del adict[kk]\n",
    "            isknown=self.unigram.get(k,0)\n",
    "            if isknown==0:\n",
    "                del self.bigram[k]\n",
    "                current=self.bigram.get(\"_UNK\",{})\n",
    "                current.update(adict)\n",
    "                self.bigram[\"_UNK\"]=current\n",
    "\n",
    "            else:\n",
    "                self.bigram[k]=adict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "class language_model():\n",
    "\n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        self.unigram={}\n",
    "        self.bigram={}\n",
    "\n",
    "        self.processfiles()\n",
    "        self.make_unknowns()\n",
    "        self.convert_to_probs()\n",
    "\n",
    "\n",
    "    def processline(self,line):\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        previous=\"__END\"\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "            current=self.bigram.get(previous,{})\n",
    "            current[token]=current.get(token,0)+1\n",
    "            self.bigram[previous]=current\n",
    "            previous=token\n",
    "\n",
    "\n",
    "    def processfiles(self):\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self.processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
    "\n",
    "\n",
    "    def convert_to_probs(self):\n",
    "\n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
    "\n",
    "\n",
    "    def get_prob(self,token,context=\"\",method=\"unigram\"):\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "        elif method==\"bigram\":\n",
    "            bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
    "            return bigram.get(token,bigram.get(\"__UNK\",0))\n",
    "\n",
    "\n",
    "    def nextlikely(self,k=1,current=\"\",method=\"unigram\"):\n",
    "        #use probabilities according to method to generate a likely next sequence\n",
    "        #choose random token from k best\n",
    "        blacklist=[\"__START\",\"__UNK\"]\n",
    "\n",
    "        if method==\"unigram\":\n",
    "            dist=self.unigram\n",
    "        else:\n",
    "            dist=self.bigram.get(current,self.bigram.get(\"__UNK\",{}))\n",
    "\n",
    "        #sort the tokens by unigram probability\n",
    "        mostlikely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n",
    "        #filter out any undesirable tokens\n",
    "        filtered=[w for (w,p) in mostlikely if w not in blacklist]\n",
    "        #choose one randomly from the top k\n",
    "        res=random.choice(filtered[:k])\n",
    "        return res\n",
    "\n",
    "    def generate(self,k=1,end=\"__END\",limit=20,method=\"bigram\"):\n",
    "        current=\"__START\"\n",
    "        tokens=[]\n",
    "        while current!=\"__END\" and len(tokens)<limit:\n",
    "            current=self.nextlikely(k=k,current=current,method=method)\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "\n",
    "    def compute_prob_line(self,line,method=\"unigram\"):\n",
    "        #this will add _start to the beginning of a line of text\n",
    "        #compute the probability of the line according to the desired model\n",
    "        #and returns probability together with number of tokens\n",
    "\n",
    "\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens[1:]):\n",
    "            acc+=math.log(self.get_prob(token,tokens[:i+1],method))\n",
    "        return acc,len(tokens[1:])\n",
    "\n",
    "\n",
    "    def compute_probability(self,filenames=[],method=\"unigram\"):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "\n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N=self.compute_prob_line(line,method=method)\n",
    "                            total_p+=p\n",
    "                            total_N+=N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "\n",
    "    def compute_perplexity(self,filenames=[],method=\"unigram\"):\n",
    "\n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "\n",
    "        p,N=self.compute_probability(filenames=filenames,method=method)\n",
    "        #print(p,N)\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp\n",
    "\n",
    "    def make_unknowns(self,known=2):\n",
    "        unknown=0\n",
    "        for (k,v) in list(self.unigram.items()):\n",
    "            if v<known:\n",
    "                del self.unigram[k]\n",
    "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
    "        for (k,adict) in list(self.bigram.items()):\n",
    "            for (kk,v) in list(adict.items()):\n",
    "                isknown=self.unigram.get(kk,0)\n",
    "                if isknown==0:\n",
    "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
    "                    del adict[kk]\n",
    "            isknown=self.unigram.get(k,0)\n",
    "            if isknown==0:\n",
    "                del self.bigram[k]\n",
    "                current=self.bigram.get(\"__UNK\",{})\n",
    "                current.update(adict)\n",
    "                self.bigram[\"__UNK\"]=current\n",
    "\n",
    "            else:\n",
    "                self.bigram[k]=adict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n",
      "Processing file 0:DYNMT10.TXT\n",
      "Processing file 1:09TOM10.TXT\n",
      "Processing file 2:PRSIT10.TXT\n",
      "Processing file 3:NWIND10.TXT\n",
      "Processing file 4:BDAPH10.TXT\n",
      "Training data unigram perplexity: 410.6950375016966\n",
      "Processing file 0:GGIRL10.TXT\n",
      "Processing file 1:SBRUN10.TXT\n",
      "Processing file 2:TARZ510.TXT\n",
      "Processing file 3:ASPRN10.TXT\n",
      "Processing file 4:TBTAS10.TXT\n",
      "Testing data unigram perplexity: 361.47935014681156\n"
     ]
    }
   ],
   "source": [
    "mylm=language_model(files=trainingfiles[:MAX_FILES])\n",
    "p=mylm.compute_perplexity()\n",
    "print(\"Training data unigram perplexity: {}\".format(p))\n",
    "p=mylm.compute_perplexity(filenames=heldoutfiles[:MAX_FILES])\n",
    "print(\"Testing data unigram perplexity: {}\".format(p))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2. Discounting for unseen combinations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "class language_model():\n",
    "\n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        self.unigram={}\n",
    "        self.bigram={}\n",
    "\n",
    "        self._processfiles()\n",
    "        self._make_unknowns()\n",
    "        self._discount()\n",
    "        self._convert_to_probs()\n",
    "\n",
    "\n",
    "    def _processline(self,line):\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        previous=\"__END\"\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "            current=self.bigram.get(previous,{})\n",
    "            current[token]=current.get(token,0)+1\n",
    "            self.bigram[previous]=current\n",
    "            previous=token\n",
    "\n",
    "\n",
    "    def _processfiles(self):\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
    "\n",
    "\n",
    "    def _convert_to_probs(self):\n",
    "\n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
    "\n",
    "\n",
    "    def get_prob(self,token,context=\"\",method=\"unigram\"):\n",
    "        if method==\"unigram\":\n",
    "            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "        elif method==\"bigram\":\n",
    "            bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
    "            big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n",
    "            lmbda=bigram[\"__DISCOUNT\"]\n",
    "            uni_p=self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "            #print(big_p,lmbda,uni_p)\n",
    "            p=big_p+lmbda*uni_p\n",
    "            return p\n",
    "\n",
    "\n",
    "    def nextlikely(self,k=1,current=\"\",method=\"unigram\"):\n",
    "        #use probabilities according to method to generate a likely next sequence\n",
    "        #choose random token from k best\n",
    "        blacklist=[\"__START\",\"__UNK\",\"__DISCOUNT\"]\n",
    "\n",
    "        if method==\"unigram\":\n",
    "            dist=self.unigram\n",
    "        else:\n",
    "            dist=self.bigram.get(current,self.bigram.get(\"__UNK\",{}))\n",
    "\n",
    "        #sort the tokens by unigram probability\n",
    "        mostlikely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n",
    "        #filter out any undesirable tokens\n",
    "        filtered=[w for (w,p) in mostlikely if w not in blacklist]\n",
    "        #choose one randomly from the top k\n",
    "        res=random.choice(filtered[:k])\n",
    "        return res\n",
    "\n",
    "    def generate(self,k=1,end=\"__END\",limit=20,method=\"bigram\"):\n",
    "        current=\"__START\"\n",
    "        tokens=[]\n",
    "        while current!=end and len(tokens)<limit:\n",
    "            current=self.nextlikely(k=k,current=current,method=method)\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "\n",
    "    def compute_prob_line(self,line,method=\"unigram\"):\n",
    "        #this will add _start to the beginning of a line of text\n",
    "        #compute the probability of the line according to the desired model\n",
    "        #and returns probability together with number of tokens\n",
    "\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens[1:]):\n",
    "            acc+=math.log(self.get_prob(token,tokens[:i+1],method))\n",
    "        return acc,len(tokens[1:])\n",
    "\n",
    "    def compute_probability(self,filenames=[],method=\"unigram\"):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "\n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N=self.compute_prob_line(line,method=method)\n",
    "                            total_p+=p\n",
    "                            total_N+=N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "\n",
    "    def compute_perplexity(self,filenames=[],method=\"unigram\"):\n",
    "\n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "\n",
    "        p,N=self.compute_probability(filenames=filenames,method=method)\n",
    "        #print(p,N)\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp\n",
    "\n",
    "    def _make_unknowns(self,known=2):\n",
    "        unknown=0\n",
    "        for (k,v) in list(self.unigram.items()):\n",
    "            if v<known:\n",
    "                del self.unigram[k]\n",
    "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
    "        for (k,adict) in list(self.bigram.items()):\n",
    "            for (kk,v) in list(adict.items()):\n",
    "                isknown=self.unigram.get(kk,0)\n",
    "                if isknown==0:\n",
    "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
    "                    del adict[kk]\n",
    "            isknown=self.unigram.get(k,0)\n",
    "            if isknown==0:\n",
    "                del self.bigram[k]\n",
    "                current=self.bigram.get(\"__UNK\",{})\n",
    "                current.update(adict)\n",
    "                self.bigram[\"__UNK\"]=current\n",
    "\n",
    "            else:\n",
    "                self.bigram[k]=adict\n",
    "\n",
    "    def _discount(self,discount=0.75):\n",
    "        #discount each bigram count by a small fixed amount\n",
    "        self.bigram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.bigram.items()}\n",
    "\n",
    "        #for each word, store the total amount of the discount so that the total is the same\n",
    "        #i.e., so we are reserving this as probability mass\n",
    "        for k in self.bigram.keys():\n",
    "            lamb=len(self.bigram[k])\n",
    "            self.bigram[k][\"__DISCOUNT\"]=lamb*discount"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n",
      "Processing file 0:DYNMT10.TXT\n",
      "Processing file 1:09TOM10.TXT\n",
      "Processing file 2:PRSIT10.TXT\n",
      "Processing file 3:NWIND10.TXT\n",
      "Processing file 4:BDAPH10.TXT\n",
      "Perplexity on training with unigram method is 410.6950375016966\n",
      "Processing file 0:DYNMT10.TXT\n",
      "Processing file 1:09TOM10.TXT\n",
      "Processing file 2:PRSIT10.TXT\n",
      "Processing file 3:NWIND10.TXT\n",
      "Processing file 4:BDAPH10.TXT\n",
      "Perplexity on training with bigram method is 60.94723288295493\n",
      "Processing file 0:GGIRL10.TXT\n",
      "Processing file 1:SBRUN10.TXT\n",
      "Processing file 2:TARZ510.TXT\n",
      "Processing file 3:ASPRN10.TXT\n",
      "Processing file 4:TBTAS10.TXT\n",
      "Perplexity on testing with unigram method is 361.47935014681156\n",
      "Processing file 0:GGIRL10.TXT\n",
      "Processing file 1:SBRUN10.TXT\n",
      "Processing file 2:TARZ510.TXT\n",
      "Processing file 3:ASPRN10.TXT\n",
      "Processing file 4:TBTAS10.TXT\n",
      "Perplexity on testing with bigram method is 72.71931865316098\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_FILES=5\n",
    "\n",
    "filesets={\"training\":trainingfiles[:MAX_FILES],\"testing\":heldoutfiles[:MAX_FILES]}\n",
    "\n",
    "\n",
    "mylm=language_model(files=filesets[\"training\"])\n",
    "methods=[\"unigram\",\"bigram\"]\n",
    "#methods=[\"bigram\"]\n",
    "\n",
    "for f,names in list(filesets.items()):\n",
    "    for m in methods:\n",
    "\n",
    "        p=mylm.compute_perplexity(filenames=names,method=m)\n",
    "\n",
    "        print(\"Perplexity on {} with {} method is {}\".format(f,m,p))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "\"of them ? ' I have to a man , but a long it would be __END\""
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.generate(k=10,method=\"bigram\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Extensions\n",
    "### 6.1. Kneser-Ney Backoff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "class language_model():\n",
    "\n",
    "    def __init__(self,trainingdir=TRAINING_DIR,files=[]):\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        self.unigram={}\n",
    "        self.bigram={}\n",
    "\n",
    "        self._processfiles()\n",
    "        self._make_unknowns()\n",
    "        self._discount()\n",
    "        self._convert_to_probs()\n",
    "\n",
    "\n",
    "    def _processline(self,line):\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        previous=\"__END\"\n",
    "        for token in tokens:\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "            current=self.bigram.get(previous,{})\n",
    "            current[token]=current.get(token,0)+1\n",
    "            self.bigram[previous]=current\n",
    "            previous=token\n",
    "\n",
    "\n",
    "    def _processfiles(self):\n",
    "        for afile in self.files:\n",
    "            print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
    "\n",
    "\n",
    "    def _convert_to_probs(self):\n",
    "\n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
    "        self.kn={k:v/sum(self.kn.values()) for (k,v) in self.kn.items()}\n",
    "\n",
    "    def get_prob(self,token,context=\"\",methodparams={}):\n",
    "        if methodparams.get(\"method\",\"unigram\")==\"unigram\":\n",
    "            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "        else:\n",
    "            if methodparams.get(\"smoothing\",\"kneser-ney\")==\"kneser-ney\":\n",
    "                unidist=self.kn\n",
    "            else:\n",
    "                unidist=self.unigram\n",
    "            bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
    "            big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n",
    "            lmbda=bigram[\"__DISCOUNT\"]\n",
    "            uni_p=unidist.get(token,unidist.get(\"__UNK\",0))\n",
    "            #print(big_p,lmbda,uni_p)\n",
    "            p=big_p+lmbda*uni_p\n",
    "            return p\n",
    "\n",
    "\n",
    "    def nextlikely(self,k=1,current=\"\",method=\"unigram\"):\n",
    "        #use probabilities according to method to generate a likely next sequence\n",
    "        #choose random token from k best\n",
    "        blacklist=[\"__START\",\"__UNK\",\"__DISCOUNT\"]\n",
    "\n",
    "        if method==\"unigram\":\n",
    "            dist=self.unigram\n",
    "        else:\n",
    "            dist=self.bigram.get(current,self.bigram.get(\"__UNK\",{}))\n",
    "\n",
    "        #sort the tokens by unigram probability\n",
    "        mostlikely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n",
    "        #filter out any undesirable tokens\n",
    "        filtered=[w for (w,p) in mostlikely if w not in blacklist]\n",
    "        #choose one randomly from the top k\n",
    "        res=random.choice(filtered[:k])\n",
    "        return res\n",
    "\n",
    "    def generate(self,k=1,end=\"__END\",limit=20,method=\"bigram\",methodparams={}):\n",
    "        if method==\"\":\n",
    "            method=methodparams.get(\"method\",\"bigram\")\n",
    "        current=\"__START\"\n",
    "        tokens=[]\n",
    "        while current!=end and len(tokens)<limit:\n",
    "            current=self.nextlikely(k=k,current=current,method=method)\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens[:-1])\n",
    "\n",
    "\n",
    "    def compute_prob_line(self,line,methodparams={}):\n",
    "        #this will add _start to the beginning of a line of text\n",
    "        #compute the probability of the line according to the desired model\n",
    "        #and returns probability together with number of tokens\n",
    "\n",
    "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
    "        acc=0\n",
    "        for i,token in enumerate(tokens[1:]):\n",
    "            acc+=math.log(self.get_prob(token,tokens[:i+1],methodparams))\n",
    "        return acc,len(tokens[1:])\n",
    "\n",
    "    def compute_probability(self,filenames=[],methodparams={}):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "\n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            p,N=self.compute_prob_line(line,methodparams=methodparams)\n",
    "                            total_p+=p\n",
    "                            total_N+=N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "\n",
    "    def compute_perplexity(self,filenames=[],methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"}):\n",
    "\n",
    "        #compute the probability and length of the corpus\n",
    "        #calculate perplexity\n",
    "        #lower perplexity means that the model better explains the data\n",
    "\n",
    "        p,N=self.compute_probability(filenames=filenames,methodparams=methodparams)\n",
    "        #print(p,N)\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp\n",
    "\n",
    "    def _make_unknowns(self,known=2):\n",
    "        unknown=0\n",
    "        for (k,v) in list(self.unigram.items()):\n",
    "            if v<known:\n",
    "                del self.unigram[k]\n",
    "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
    "        for (k,adict) in list(self.bigram.items()):\n",
    "            for (kk,v) in list(adict.items()):\n",
    "                isknown=self.unigram.get(kk,0)\n",
    "                if isknown==0:\n",
    "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
    "                    del adict[kk]\n",
    "            isknown=self.unigram.get(k,0)\n",
    "            if isknown==0:\n",
    "                del self.bigram[k]\n",
    "                current=self.bigram.get(\"__UNK\",{})\n",
    "                current.update(adict)\n",
    "                self.bigram[\"__UNK\"]=current\n",
    "\n",
    "            else:\n",
    "                self.bigram[k]=adict\n",
    "\n",
    "    def _discount(self,discount=0.75):\n",
    "        #discount each bigram count by a small fixed amount\n",
    "        self.bigram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.bigram.items()}\n",
    "\n",
    "        #for each word, store the total amount of the discount so that the total is the same\n",
    "        #i.e., so we are reserving this as probability mass\n",
    "        for k in self.bigram.keys():\n",
    "            lamb=len(self.bigram[k])\n",
    "            self.bigram[k][\"__DISCOUNT\"]=lamb*discount\n",
    "\n",
    "        #work out kneser-ney unigram probabilities\n",
    "        #count the number of contexts each word has been seen in\n",
    "        self.kn={}\n",
    "        for (k,adict) in self.bigram.items():\n",
    "            for kk in adict.keys():\n",
    "                self.kn[kk]=self.kn.get(kk,0)+1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DYNMT10.TXT\n",
      "Processing 09TOM10.TXT\n",
      "Processing PRSIT10.TXT\n",
      "Processing NWIND10.TXT\n",
      "Processing BDAPH10.TXT\n",
      "Processing COTRT10.TXT\n",
      "Processing POISN10.TXT\n",
      "Processing RNDBY10.TXT\n",
      "Processing LAMEP10.TXT\n",
      "Processing OAKDA10.TXT\n",
      "Processing file 0:DYNMT10.TXT\n",
      "Processing file 1:09TOM10.TXT\n",
      "Processing file 2:PRSIT10.TXT\n",
      "Processing file 3:NWIND10.TXT\n",
      "Processing file 4:BDAPH10.TXT\n",
      "Processing file 5:COTRT10.TXT\n",
      "Processing file 6:POISN10.TXT\n",
      "Processing file 7:RNDBY10.TXT\n",
      "Processing file 8:LAMEP10.TXT\n",
      "Processing file 9:OAKDA10.TXT\n",
      "Perplexity on training with <unigram,no smoothing> method is 467.7230096346662\n",
      "Processing file 0:DYNMT10.TXT\n",
      "Processing file 1:09TOM10.TXT\n",
      "Processing file 2:PRSIT10.TXT\n",
      "Processing file 3:NWIND10.TXT\n",
      "Processing file 4:BDAPH10.TXT\n",
      "Processing file 5:COTRT10.TXT\n",
      "Processing file 6:POISN10.TXT\n",
      "Processing file 7:RNDBY10.TXT\n",
      "Processing file 8:LAMEP10.TXT\n",
      "Processing file 9:OAKDA10.TXT\n",
      "Perplexity on training with <bigram,katz> method is 69.64855700160476\n",
      "Processing file 0:DYNMT10.TXT\n",
      "Processing file 1:09TOM10.TXT\n",
      "Processing file 2:PRSIT10.TXT\n",
      "Processing file 3:NWIND10.TXT\n",
      "Processing file 4:BDAPH10.TXT\n",
      "Processing file 5:COTRT10.TXT\n",
      "Processing file 6:POISN10.TXT\n",
      "Processing file 7:RNDBY10.TXT\n",
      "Processing file 8:LAMEP10.TXT\n",
      "Processing file 9:OAKDA10.TXT\n",
      "Perplexity on training with <bigram,kneser-ney> method is 70.66268871977285\n",
      "Processing file 0:GGIRL10.TXT\n",
      "Processing file 1:SBRUN10.TXT\n",
      "Processing file 2:TARZ510.TXT\n",
      "Processing file 3:ASPRN10.TXT\n",
      "Processing file 4:TBTAS10.TXT\n",
      "Processing file 5:TARZ410.TXT\n",
      "Processing file 6:LWMEN10.TXT\n",
      "Processing file 7:HHOHG10.TXT\n",
      "Processing file 8:ALICE30.TXT\n",
      "Processing file 9:BLCKA10.TXT\n",
      "Perplexity on testing with <unigram,no smoothing> method is 405.5879279363114\n",
      "Processing file 0:GGIRL10.TXT\n",
      "Processing file 1:SBRUN10.TXT\n",
      "Processing file 2:TARZ510.TXT\n",
      "Processing file 3:ASPRN10.TXT\n",
      "Processing file 4:TBTAS10.TXT\n",
      "Processing file 5:TARZ410.TXT\n",
      "Processing file 6:LWMEN10.TXT\n",
      "Processing file 7:HHOHG10.TXT\n",
      "Processing file 8:ALICE30.TXT\n",
      "Processing file 9:BLCKA10.TXT\n",
      "Perplexity on testing with <bigram,katz> method is 87.67632700801435\n",
      "Processing file 0:GGIRL10.TXT\n",
      "Processing file 1:SBRUN10.TXT\n",
      "Processing file 2:TARZ510.TXT\n",
      "Processing file 3:ASPRN10.TXT\n",
      "Processing file 4:TBTAS10.TXT\n",
      "Processing file 5:TARZ410.TXT\n",
      "Processing file 6:LWMEN10.TXT\n",
      "Processing file 7:HHOHG10.TXT\n",
      "Processing file 8:ALICE30.TXT\n",
      "Processing file 9:BLCKA10.TXT\n",
      "Perplexity on testing with <bigram,kneser-ney> method is 90.11458251161733\n"
     ]
    }
   ],
   "source": [
    "MAX_FILES=10\n",
    "\n",
    "filesets={\"training\":trainingfiles[:MAX_FILES],\"testing\":heldoutfiles[:MAX_FILES]}\n",
    "\n",
    "\n",
    "mylm=language_model(files=filesets[\"training\"])\n",
    "methods=[{\"method\":\"unigram\"},{\"method\":\"bigram\",\"smoothing\":\"katz\"},{\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"}]\n",
    "#methods=[\"bigram\"]\n",
    "\n",
    "for f,names in list(filesets.items()):\n",
    "    for m in methods:\n",
    "\n",
    "        p=mylm.compute_perplexity(filenames=names,methodparams=m)\n",
    "\n",
    "        print(\"Perplexity on {} with <{},{}> method is {}\".format(f,m[\"method\"],m.get(\"smoothing\",\"no smoothing\"),p))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "\"`` I have n't know that the\""
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylm.generate(k=5,method=\"bigram\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}